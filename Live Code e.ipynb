{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-8-33c29975fd52>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-33c29975fd52>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    url = 'https://github.com/LeDataSciFi/lectures-spr2020/blob/master/assignment_data/Fannie_Mae_Plus_Data.gzip?raw=true'\u001b[0m\n\u001b[0m                                                                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = 'https://github.com/LeDataSciFi/lectures-spr2020/blob/master/assignment_data/Fannie_Mae_Plus_Data.gzip?raw=true' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Cardinal sin of data. \n",
    "**Having data in the training sample that you wouldn't have for real world predictions**\n",
    "Examples\n",
    "1. y is explicitly in X (yikes)\n",
    "2. y is a 2018 variable, but there is a 2019 variable in X\n",
    "3. subtle: y is loan default, but X contains employee ID and some employees are brought in to handle trouble-loans (if you include it, the firm can't use the model to deploy the trouble-loan specialists)\n",
    "4. if out-of-sample predicted stock movements have R2 above 10%... unlikely! (or: you'll be richer than Bezos soon)\n",
    "5. this code below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-bfea6777b022>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-bfea6777b022>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    import #a bunch of sklearn stuff\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import #a bunch of sklearn stuff\n",
    "X, y = #load data\n",
    "X = transform(X) # imputation, encode cat vars, standardize\n",
    "\n",
    "# or this:\n",
    "cross_validate(model,X,y)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What's the problem here?**\n",
    "\n",
    "**A: `transform(X)` used the whole dataset, so the X_training data was altered using info from X_test** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-39f7a9374bc4>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-39f7a9374bc4>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    x1 sample\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "x1 sample\n",
    "1 training\n",
    "1 training\n",
    "\n",
    "2 test\n",
    "1 test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Data Leakage\n",
    "\n",
    "-Preventing 1-4: Be very familiar with the data and how it was collected and built\n",
    "-Preventing 5: D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Data Leakage\n",
    "\n",
    "- Preventing 1-4: Be very familiar with the data and how it was collected and built \n",
    "- Preventing 5: Do your data prep _**within**_ CV folds and where the transformations are done using only info from the training \n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StratifiedKFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a67549597ad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# loop over folds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# .split() yields the indices in train/test sets. use those to get\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# the x/y vars for each separated out:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StratifiedKFold' is not defined"
     ]
    }
   ],
   "source": [
    "# loop over folds \n",
    "for train_index, test_index in StratifiedKFold(n_splits=5).split(X,y):\n",
    "\n",
    "    # .split() yields the indices in train/test sets. use those to get \n",
    "    # the x/y vars for each separated out:\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prep_methods' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-bf16d8a9f8e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# e.g. figure out means/std in Xtrain so we can impute/std\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprep_methods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;31m# \"fit\" the transform means \"estimate (like in training a model) what to do\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mXtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_methods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# apply those to Xtrain to impute and std\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prep_methods' is not defined"
     ]
    }
   ],
   "source": [
    "###################################################################\n",
    "    # NEW: do the data prep inside this fold, only using training data \n",
    "    ###################################################################\n",
    "\n",
    "    # e.g. figure out means/std in Xtrain so we can impute/std\n",
    "prep_methods.fit(Xtrain)                 # \"fit\" the transform means \"estimate (like in training a model) what to do\"\n",
    "Xtrain = prep_methods.transform(Xtrain)  # apply those to Xtrain to impute and std\n",
    "    \n",
    "    # fit/estimate, predict OOS, evaluate and store\n",
    "model.fit(X_train,y_train)\n",
    "    \n",
    "    ###################################################################\n",
    "    # NEW: transform the test data the same... \n",
    "    ###################################################################\n",
    "    \n",
    "X_test = prep_methods.transform(X_test)  # apply TEST data the FIT from the TRAIN data \n",
    "    \n",
    "y_predict = model.predict(X_test)\n",
    "accuracy.append(   accuracy_score(y_test, y_predict)      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00600004, 0.00103688, 0.00102925, 0.00083995, 0.00111127]),\n",
       " 'score_time': array([0.00100589, 0.00035405, 0.00037384, 0.00032997, 0.00031686]),\n",
       " 'test_score': array([0.96666667, 0.96666667, 0.96666667, 0.93333333, 1.        ])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import svm\n",
    "\n",
    "iris = load_iris() # data\n",
    "\n",
    "# set up the pipeline, which will, given a set of observations \n",
    "# 1. fit and apply these steps to the training fold\n",
    "# 2. in the testing fold, apply the transform and model to predict (no estimation)\n",
    "\n",
    "classifier_pipeline = make_pipeline(\n",
    "                                    preprocessing.StandardScaler(),  # clean the data\n",
    "                                    svm.SVC(C=1)                     # model\n",
    "                                    )\n",
    "\n",
    "cross_validate(classifier_pipeline, iris.data, iris.target, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00149083, 0.00136876, 0.00115299, 0.00143266, 0.00128484]),\n",
       " 'score_time': array([0.00044703, 0.00055909, 0.00039315, 0.0004003 , 0.00041223]),\n",
       " 'test_score': array([0.96666667, 0.96666667, 0.96666667, 0.93333333, 1.        ])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question 1: try this with a Nearest Neighbors Classifier (5 min)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_pipe = make_pipeline(\n",
    "                        preprocessing.StandardScaler(),  # clean the data\n",
    "                        KNeighborsClassifier()           # model\n",
    "                        )\n",
    "\n",
    "cross_validate(classifier_pipeline, iris.data, iris.target, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00579882, 0.00262713, 0.00456595, 0.00313807, 0.00338316]),\n",
       " 'score_time': array([0.0036521 , 0.00203395, 0.0023911 , 0.00197887, 0.00191689]),\n",
       " 'test_score': array([0.9       , 0.96666667, 0.9       , 0.96666667, 1.        ])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question 2: load this altered dataset and add a step to impute the missing values with the column mean\n",
    "\n",
    "iris2 = load_iris()\n",
    "X2 = pd.DataFrame(iris2.data)\n",
    "X2.columns = [1,2,3,4]\n",
    "X2[2] = X2[2].sample(frac=0.5,random_state=14)\n",
    "X2[2].describe()\n",
    "iris2.data = X2\n",
    "\n",
    "# print the scores using IRIS2.data (not iris.data)\n",
    "# this produces an error because of the missing values!\n",
    "# cross_validate(knn_pipe, iris2.data, iris.target, cv=5)\n",
    "\n",
    "# so add an imputation step to the pipeline! (5 min, use lecture page!)\n",
    "# so add an imputation step to the pipeline! (5 min, use lecture page!)\n",
    "from sklearn.impute import SimpleImputer\n",
    "knn_pipe2 = make_pipeline(\n",
    "                        SimpleImputer(strategy='mean'),  # fill missing values\n",
    "                        preprocessing.StandardScaler(),  # clean the data\n",
    "                        KNeighborsClassifier()           # model\n",
    "                        )\n",
    "\n",
    "cross_validate(knn_pipe2, iris2.data, iris.target, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize a model-here, KNN, with 'GridSearch CV'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('simpleimputer',\n",
       "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                               missing_values=nan, strategy='mean',\n",
       "                               verbose=0)),\n",
       "                ('standardscaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('kneighborsclassifier',\n",
       "                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "                                      metric='minkowski', metric_params=None,\n",
       "                                      n_jobs=None, n_neighbors=5, p=2,\n",
       "                                      weights='uniform'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_pipe2 #look at all the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search will let you specify all the parameters of the model\n",
    "#you want to tweak, and the values you want to try\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#set up parrameter grid to try\n",
    "#the parameter grid is a dictionary where key:value pairs are built like:\n",
    "#stepName<two underlines>paramName: [list of settings to. try]\n",
    "\n",
    "param_grid= {'kneighborsclassifier__n_neighbors': [1,5,6,7,8,9,10]}\n",
    "\n",
    "#like a normal estimator, this has not yett been applied to any data\n",
    "grid=GridSearchCV(knn_pipe2, param_grid=param_grid)\n",
    "grid.fit(iris2.data, iris.target)\n",
    "grid.best_params_\n",
    "\n",
    "#now save that pipeline as a model object:\n",
    "optimal_knn_model= grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kneighborsclassifier__n_neighbors': 6,\n",
       " 'standardscaler__with_mean': 'True',\n",
       " 'standardscaler__with_std': 'True'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#question 3: add to the param grid to check if. we should change these two params\n",
    "#.  StandardScaler (with_mean=True, with_std=True)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#set up parrameter grid to try\n",
    "#the parameter grid is a dictionary where key:value pairs are built like:\n",
    "#stepName<two underlines>paramName: [list of settings to. try]\n",
    "\n",
    "param_grid= {'kneighborsclassifier__n_neighbors': [1,5,6,7,8,9,10],\n",
    "             'standardscaler__with_mean':['True','False'],\n",
    "             'standardscaler__with_std':['True','False']}\n",
    "\n",
    "#like a normal estimator, this has not yett been applied to any data\n",
    "grid=GridSearchCV(knn_pipe2, param_grid=param_grid)\n",
    "grid.fit(iris2.data, iris.target)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# use confusion_matrix see exactly model gets predictions wrong\n",
    "#################################################################\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_confusion_matrix(optimal_knn_model, Xtest, ytest,   # model and test data\n",
    "                      display_labels=iris.target_names,  # labels\n",
    "                      cmap=plt.cm.Blues,                 # colors\n",
    "                      normalize=None)                    # turns on/off fractions (within row) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final Summary\n",
    "\n",
    "- We've now seen more post model diagnostics \n",
    "- We can specify the models in `make_pipeline` alongside data cleaning/preprocessing steps that improve model performance without introducing data leakage. \n",
    "- There are many imputation, and scaling methods available in `sklearn`, and which one you use depends on the use-case. (Read about and try several!)\n",
    "- Your pipeline for the assignment will be more complicated if you want to include categorical vars\n",
    "- You can optimize all of the parameters throughout your pipeline using `GridSearchCV`\n",
    "    - `GridSearchCV` also allows you to specify how you create folds\n",
    "    - Which leads us to...\n",
    "\n",
    "**LAST BIG POINT:** \n",
    "- Must of your projects involve an important time series dimension. (Ex: predicting stock returns) \n",
    "- In these cases, `KFold` and `StratifiedKFold` won't work (you can't have 1985 in the test sample)\n",
    "- See: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
